{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport category_encoders\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport datetime\nimport gc,os\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\nimport pickle\n\nfrom mlxtend.regressor import StackingRegressor\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nos.chdir('../input/')\nweather_train=pd.read_csv('ashrae-energy-prediction/weather_train.csv')\nweather_test=pd.read_csv('ashrae-energy-prediction/weather_test.csv')\ntrain=pd.read_csv('ashrae-energy-prediction/train.csv')\ntest=pd.read_csv('ashrae-energy-prediction/test.csv')\nbuilding_metadata=pd.read_csv('ashrae-energy-prediction/building_metadata.csv')\n\n#building_metadata,weather_test and weather_train have null values which have to be fixed.\n#next look for outliers and remove them\ntrain=train[train['building_id']!=1099]\ntrain=train.query('not(building_id<=104 & meter==0 & timestamp<=\"2016-05-20\")')\n\ndef fill_weather_dataset(weather_train)->pd.DataFrame:\n    time_format = '%Y-%m-%d %H:%M:%S'\n    start_date = datetime.datetime.strptime(weather_train['timestamp'].min(), time_format)\n    end_date = datetime.datetime.strptime(weather_train['timestamp'].max(), time_format)\n    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n\n    for site_id in range(16):\n        site_hours = np.array(weather_train[weather_train['site_id'] == site_id]['timestamp'])\n        new_rows = pd.DataFrame(np.setdiff1d(hours_list, site_hours), columns=['timestamp'])\n        new_rows['site_id'] = site_id\n        weather_train = pd.concat([weather_train, new_rows], sort=True)\n        weather_train = weather_train.reset_index(drop=True)  \n    # Add new Features\n    weather_train[\"datetime\"] = pd.to_datetime(weather_train[\"timestamp\"])\n    weather_train[\"day\"] = weather_train[\"datetime\"].dt.day\n    weather_train[\"week\"] = weather_train[\"datetime\"].dt.week\n    weather_train[\"month\"] = weather_train[\"datetime\"].dt.month\n     # Reset Index for Fast Update\n    weather_train = weather_train.set_index(['site_id','day','month'])\n    air_temperature_filler = pd.DataFrame(weather_train.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n    weather_train.update(air_temperature_filler,overwrite=False)\n    # Step 1\n    cloud_coverage_filler = weather_train.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n    # Step 2\n    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n\n    weather_train.update(cloud_coverage_filler,overwrite=False)\n    dew_temperature_filler = pd.DataFrame(weather_train.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n    weather_train.update(dew_temperature_filler,overwrite=False)\n\n    # Step 1\n    sea_level_filler = weather_train.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n    # Step 2\n    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n\n    weather_train.update(sea_level_filler,overwrite=False)\n    wind_direction_filler =  pd.DataFrame(weather_train.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n    weather_train.update(wind_direction_filler,overwrite=False)\n\n    wind_speed_filler =  pd.DataFrame(weather_train.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n    weather_train.update(wind_speed_filler,overwrite=False)\n\n    # Step 1\n    precip_depth_filler = weather_train.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n    # Step 2\n    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n\n    weather_train.update(precip_depth_filler,overwrite=False)\n\n    weather_train = weather_train.reset_index()\n    weather_train = weather_train.drop(['datetime','day','week','month'],axis=1)\n        \n    return weather_train\n\ndef reduce_mem_usage(df, use_float16=False)->pd.DataFrame:\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef features_engineering(df)->pd.DataFrame:\n    \n    # Sort by timestamp\n    df.sort_values(\"timestamp\")\n    df.reset_index(drop=True)\n    \n    # Add more features\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n                    \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n                    \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n                    \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n                    \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n                    \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n                    \"2019-01-01\"]\n    df[\"is_holiday\"] = (df.timestamp.isin(holidays)).astype(int)\n    df['square_feet'] =  np.log1p(df['square_feet'])\n    \n    # Remove Unused Columns\n    drop = [\"timestamp\",\"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\"year_built\",\"floor_count\"]\n    df = df.drop(drop, axis=1)\n    gc.collect()\n    \n    # Encode Categorical Data\n    le = LabelEncoder()\n    df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n    \n    return df\n\nweather_train = fill_weather_dataset(weather_train)\ntrain= reduce_mem_usage(train,use_float16=True)\nbuilding_metadata = reduce_mem_usage(building_metadata,use_float16=True)\nweather_train = reduce_mem_usage(weather_train,use_float16=True)\ntrain = train.merge(building_metadata,on='building_id',how='left')\ntrain = train.merge(weather_train,how='left',on=['site_id','timestamp'])\ndel weather_train\ngc.collect()\ntrain = features_engineering(train)\n\ntarget = np.log1p(train[\"meter_reading\"])\ntrain_df = train.drop(['meter_reading'], axis = 1)\ndel train\ngc.collect()\ntrain_df.head()\n\ncategorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\"]\nce = category_encoders.CountEncoder(cols=categorical_features)\nce.fit(train_df)\ntrain_df = ce.transform(train_df)\nN_train = train_df.shape[0]\nfor feature in categorical_features:\n    train_df[feature] = train_df[feature]/N_train\n    \n# Missing data imputation\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(train_df)\ntrain_df = imputer.transform(train_df)\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nlm=LinearRegression()\nkf = KFold(n_splits=2,shuffle=False)\nmodels = []\nfor train_index, val_index in kf.split(train_df):\n    train_features = train_df[train_index]\n    train_target = target[train_index]    \n    val_features = train_df[val_index]\n    val_target = target[val_index]\n    model = LinearRegression()    \n    model.fit(np.array(train_features),np.array(train_target))\n    models.append(model)        \n    val_pred = model.predict(val_features)\n    print(np.sqrt(mean_squared_error(val_target, val_pred)))\n    del train_features, train_target, val_features, val_target\n\ndel train_df, target    \ngc.collect()\n\nlightgbm = LGBMRegressor(objective='regression', learning_rate=0.05, num_leaves=1024,\n    feature_fraction=0.8, bagging_fraction=0.9, bagging_freq=5) \n\nridge = Ridge(alpha=0.3)\nlasso = Lasso(alpha=0.3)\n\nkf = KFold(n_splits=2,shuffle=False)\nmodels = []\nfor train_index, val_index in kf.split(train_df):\n    train_features = train_df[train_index]\n    train_target = target[train_index]\n    \n    val_features = train_df[val_index]\n    val_target = target[val_index]\n    \n    model = StackingRegressor(regressors=(lightgbm, ridge, lasso),\n        meta_regressor=lightgbm, use_features_in_secondary=True)    \n    model.fit(np.array(train_features),np.array(train_target))\n    models.append(model)\n    val_pred = model.predict(val_features)\n    print(np.sqrt(mean_squared_error(val_target, val_pred)))\n    del train_features, train_target, val_features, val_target\n\ndel train_df, target    \ngc.collect()\n#test\nrow_ids = test['row_id']\ntest.drop('row_id', axis=1, inplace=True)\nweather_test = fill_weather_dataset(weather_test)\n# Memory optimization\ntest = reduce_mem_usage(test, use_float16=True)\nweather_test = reduce_mem_usage(weather_test, use_float16=True)\n# Merge test data\ntest = test.merge(building_metadata, on='building_id', how='left')\ntest = test.merge(weather_test, on=['site_id', 'timestamp'], how='left')\n\ndel building_metadata\ndel weather_test\ngc.collect()\n\n# Test data processing\ntest = features_engineering(test)\n\ntest = ce.transform(test)\nfor feature in categorical_features:\n    test[feature] = test[feature] / N_train\n\ntest = imputer.transform(test)\n\n# Make predictions\npredictions = 0\nfor model in models:\n    predictions += np.expm1(model.predict(np.array(test))) / len(models)\n    del model; gc.collect()\n\ndel test, models; gc.collect()\n\n\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'row_id': row_ids,\n    'meter_reading': np.clip(predictions, 0, a_max=None)\n})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False, float_format='%.4f')'''\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}